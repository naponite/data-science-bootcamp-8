---
title: "Machine learning to predict India house price"
author: "Napon_Tah"
date: "2023-08-14"
output:
  html_document: default
  pdf_document: default
---

## Introduction

Develop machine learning model predict India house price


#### Data source
file: House Price India.xlsx

sheet: 2017 (sheet 2)

record: 2974 records

url: https://data.world/dataindianset2000/house-price-india


#### Implementation detail
Data split:  train:test = 80:20

Model: Linear Regression 

Resampling method: Bootstrapped (25 reps) 


## 1. Study significant variables

```{r}
library(tidyverse)
library(caret)
library(dplyr)
library(readxl)
library(ggplot2)

df <- read_excel("/cloud/project/ML/House Price India.xlsx", sheet = 2) 

# pre train model to find significant variable

pre_model <- train(Price ~ .,
               data = df,
               method = "lm")

varImp(pre_model)
summary(pre_model)


```

## 2. Data preparation
#### 2.1 Filter significant variables

```{r}
prep_df <- df %>%
  select ( bdrm = `number of bedrooms`,
           bhrm = `number of bathrooms`,
           lvar = `living area`,
           flr = `number of floors`,
           w_frnt = `waterfront present`,
           n_view = `number of views`,
           cond = `condition of the house`,
           grad = `grade of the house`,
           year = `Built Year`,
           post = `Postal Code`,
           lat = Lattitude,
           long = Longitude,
           price = Price)

```

#### 2.2 Normalize price distribution
Due to the price is right skew distribution, 

```{r}
# check price distribution
ggplot(data = prep_df , aes(price))+
  geom_histogram(col = "black", fill = "blue")+
  theme_minimal()

```

log() function will be applied to normalize the data distribution for accurate model building.

```{r}
# normalize skew distribution by take log
prep_df_log <- prep_df
prep_df_log$price <- prep_df$price %>%
  log() # 

ggplot(data = prep_df_log , aes(price))+
  geom_histogram(col = "black", fill = "blue")+
  theme_minimal()
```

## 3. Model developing
#### 3.1 Train Test split
```{r}
# 1. train test split
split_data <- function(df, train_size=0.8) {
  set.seed(24)
  n <- nrow(df)
  id <- sample(1:n, size = n*train_size)
  train_df <- df[id , ]
  test_df <- df[-id , ]
  list (train = train_df,
        test = test_df)
}

prep_data <- split_data(prep_df)
train_data <- prep_data[[1]]
test_data <- prep_data[[2]]

# normalize price distribution before train model
train_data_log <- train_data
train_data_log$price <- train_data$price %>%
  log()

test_data_log <- test_data
test_data_log$price <- test_data$price %>%
  log()

```


#### 3.2 Train model

```{r}

# 2. train model
model <- train(price ~ .,
               data = train_data_log,
               method = "lm")

varImp(model)
summary(model)

# predict train price with out log (use exp() to convert back)
p_train_log <- predict(model, newdata=train_data_log) 
p_train <- p_train_log %>%
  exp()


```


#### 3.3 Model scoring

```{r}
# 3. score/ predict new data (test/ unseen data)

p_test_log <- predict(model, newdata=test_data_log) 
p_test <- p_test_log %>%
  exp()
```


#### 3.4 Model evaluation
```{r}
# 4. evaluate model

cal_mae <- function(actual, pred) {
  error <- actual - pred
  mean(abs(error))
  
}

cal_mse <- function(actual, pred) {
  error <- actual - pred
  mean((error)**2)
  
}

cal_rmse <- function(actual, pred) {
  error <- actual - pred
  sqrt(mean((error)**2))
  
}

## Evaluate model with train data (normal price)
act1 <- train_data$price
pred1 <- p_train

mae_train <- cal_mae(act1, pred1)
mse_train <- cal_mse(act1, pred1)
rmse_train <- cal_rmse(act1, pred1)

## Evaluate model with test data (normal price)
act2 <- test_data$price
pred2 <- p_test

mae_test <- cal_mae(act2, pred2)
mse_test <- cal_mse(act2, pred2)
rmse_test <- cal_rmse(act2, pred2)

## Evaluate model with train data (log price)
act3 <- train_data_log$price
pred3 <- p_train_log

mae_train_log <- cal_mae(act3, pred3)
mse_train_log <- cal_mse(act3, pred3)
rmse_train_log <- cal_rmse(act3, pred3)

## Evaluate model with test data (log price)
act4 <- test_data_log$price
pred4 <- p_test_log

mae_test_log <- cal_mae(act4, pred4)
mse_test_log <- cal_mse(act4, pred4)
rmse_test_log <- cal_rmse(act4, pred4)

## create compare data frame
measure <- c("mae", "mse", "rmse")
train_price <- c(mae_train, mse_train, rmse_train)
test_price <- c(mae_test, mse_test, rmse_test)
train_price_log <- c(mae_train_log, mse_train_log, rmse_train_log)
test_price_log <- c(mae_test_log, mse_test_log, rmse_test_log)

sum_result <- data.frame(measure, 
                         train_price,
                         test_price,
                         train_price_log,
                         test_price_log)

sum_result

```

## 4. Compare model performance between normalize data vs not normalize data

```{r}
# train model with out price normalization (not take log)

model_nolog <- train(price ~ .,
               data = train_data,
               method = "lm")


p_train2 <- predict(model_nolog, newdata=train_data) 

p_test2 <- predict(model_nolog, newdata=test_data) 


## Evaluate model_nolog with train data (normal price)
act5 <- train_data$price
pred5 <- p_train2

mae_train_nolog <- cal_mae(act5, pred5)
mse_train_nolog <- cal_mse(act5, pred5)
rmse_train_nolog <- cal_rmse(act5, pred5)

## Evaluate model_nolog with test data (normal price)
act6 <- test_data$price
pred6 <- p_test2

mae_test_nolog <- cal_mae(act6, pred6)
mse_test_nolog <- cal_mse(act6, pred6)
rmse_test_nolog <- cal_rmse(act6, pred6)

```



Model performance with normalized data
```{r}
model_norm_price <- c(model$results[2:4])
model_norm_price


```

Model performance without normalized data (not take log)
```{r}

model_skew_price <- c(model_nolog$results[2:4])
model_skew_price

```

### Conclusion
Model performance with normalized price distribution is higher than skew distribution price using (considered in term of Rsquared)

